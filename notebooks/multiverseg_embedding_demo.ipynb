{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df013d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The yamlmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext yamlmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiverSeg embedding sandbox\n",
    "\n",
    "Quick testbed to extract a single vector embedding from the MultiverSeg encoder using global avg/max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4da5cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "repo_root = Path(\"/data/ddmg/mvseg-ordering/\")\n",
    "for path in [repo_root, repo_root / \"UniverSeg\", repo_root / \"MultiverSeg\"]:\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.append(str(path))\n",
    "\n",
    "from experiments.dataset.mega_medical_dataset import MegaMedicalDataset\n",
    "from experiments.encoders.multiverseg_encoder import MultiverSegEncoder\n",
    "from experiments.encoders.clip import CLIPEncoder\n",
    "from experiments.encoders.vit import ViTEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb238a5",
   "metadata": {},
   "source": [
    "## Pick an encoder\n",
    "Uncomment the encoder you want to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No updates to index\n",
      "Filtered task_df: 1248\n",
      "got task df: 1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/mvseg-ordering/experiments/dataset/multisegment2d.py:156: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"label_type\"].fillna(\"soft\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_datasets: 1248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a MegaMedical target. Index 0 is a convenient default; adjust if needed.\n",
    "dataset_target = 12\n",
    "dataset_split = \"train\"\n",
    "dataset_size = 4  # small subset for a quick sanity check\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ds = MegaMedicalDataset(\n",
    "    dataset_target=dataset_target,\n",
    "    split=dataset_split,\n",
    "    dataset_size=dataset_size,\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b2e7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1, 128, 128)\n",
      "Label shape: (1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Grab one sample (image, mask)\n",
    "img, lbl = ds[0]\n",
    "print(\"Image shape:\", tuple(img.shape))\n",
    "print(\"Label shape:\", tuple(lbl.shape))\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "img_b = img.to(device)\n",
    "lbl_b = lbl.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac967c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb2868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/users/gtorpey/envs/mvseg-ordering-env/lib/python3.10/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MultiverSeg encoder (default)\n",
    "enc = MultiverSegEncoder(\n",
    "    pooling=\"gap\",\n",
    ").to(device).eval()\n",
    "\n",
    "# CLIP encoder\n",
    "clip = CLIPEncoder(\n",
    "    model_name=\"ViT-B-32\",\n",
    "    pretrained=\"openai\",\n",
    ").to(device).eval()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # ViT encoder\n",
    "vit = ViTEncoder(\n",
    "    model_name=\"vit_b_16\",\n",
    "    pretrained=True,\n",
    ").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a433ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(img_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15bdbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 512)\n",
      "First 8 dims: [ 0.00735284 -0.01650549  0.00367393 -0.01733396 -0.0033046  -0.02681194\n",
      "  0.00133051  0.01993955]\n",
      "Embedding shape: (1, 768)\n",
      "First 8 dims: [ 0.0122594   0.07449619  0.05001451 -0.09478112  0.02037749  0.00559178\n",
      " -0.01800939  0.02084701]\n",
      "Embedding shape: (1, 256)\n",
      "First 8 dims: [ 0.01669559 -0.04529885  0.0234932  -0.00304918 -0.04687631 -0.03542258\n",
      "  0.01770178 -0.02281269]\n"
     ]
    }
   ],
   "source": [
    "# Compute embedding (global avg + max pooled, L2-normalized)\n",
    "with torch.no_grad():\n",
    "    emb = clip(img_b)\n",
    "\n",
    "print(\"Embedding shape:\", tuple(emb.shape))\n",
    "print(\"First 8 dims:\", emb[0, :8].cpu().numpy())\n",
    "\n",
    "# Compute embedding (global avg + max pooled, L2-normalized)\n",
    "with torch.no_grad():\n",
    "    emb = vit(img_b)\n",
    "\n",
    "print(\"Embedding shape:\", tuple(emb.shape))\n",
    "print(\"First 8 dims:\", emb[0, :8].cpu().numpy())\n",
    "\n",
    "# Compute embedding (global avg + max pooled, L2-normalized)\n",
    "with torch.no_grad():\n",
    "    emb = enc(img_b)\n",
    "\n",
    "print(\"Embedding shape:\", tuple(emb.shape))\n",
    "print(\"First 8 dims:\", emb[0, :8].cpu().numpy())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
