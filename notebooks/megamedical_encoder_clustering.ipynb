{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaMedical multi-dataset embedding clustering\n",
    "\n",
    "Load images from multiple MegaMedical tasks, embed with 5 encoders, then visualize PCA and k-means clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Ensure repo and vendored deps are on path\n",
    "repo_root = Path(\"/data/ddmg/mvseg-ordering/\")\n",
    "for path in [repo_root, repo_root / \"UniverSeg\", repo_root / \"MultiverSeg\"]:\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.append(str(path))\n",
    "\n",
    "from experiments.dataset.mega_medical_dataset import MegaMedicalDataset\n",
    "from experiments.encoders.multiverseg_encoder import MultiverSegEncoder\n",
    "from experiments.encoders.clip import CLIPEncoder\n",
    "from experiments.encoders.vit import ViTEncoder\n",
    "from experiments.encoders.dinov2 import DinoV2Encoder\n",
    "from experiments.encoders.medsam import MedSAMEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "\n",
    "# Pick device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Use MegaMedical target indices for easy editing.\n",
    "# Each entry is (target_index, n_samples).\n",
    "# Edit indices + counts to control which datasets and how many images.\n",
    "mega_targets = [\n",
    "    (0, 3),\n",
    "    (29, 3),\n",
    "    (12, 4),\n",
    "]\n",
    "\n",
    "# Random seed (used for sampling within each target dataset)\n",
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No updates to index\n",
      "Filtered task_df: 1248\n",
      "got task df: 1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/mvseg-ordering/experiments/dataset/multisegment2d.py:156: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"label_type\"].fillna(\"soft\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_datasets: 1248\n",
      "[11 19 24 21  7  1 12 22  2  3 13 14  5 18  8  6 10 23  4  9 16  0 17 15\n",
      " 20]\n",
      "No updates to index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/mvseg-ordering/experiments/dataset/multisegment2d.py:156: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"label_type\"].fillna(\"soft\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered task_df: 1248\n",
      "got task df: 1248\n",
      "target_datasets: 1248\n",
      "[245 166 240  21  45  25 163  61 236  10 102 257  38 118 242  60 156 183\n",
      " 233  85  36 244   1 221  42  87 246 199 250 220  90 202 215  67 134  41\n",
      " 149 186  70 146 253 141 112 173 108 212 175 140 206 135 160 180 101  35\n",
      " 237 205  89 195  91 241 208 116  39   2 182 104 103  32 138  44 159 259\n",
      " 151  17 153  93  71 235  30  47 203 225 181  73 193  72 258 172  82 254\n",
      " 248 152  18 228  15  24 158  92 210  96  75 232  27 143 179   3 105 128\n",
      "  23  22  57 127 256 214  83  74 223 136 229 167  63 211  68 190  50 122\n",
      "   7 123 157 107   5 168 191 184  46  55 189 219 100 209 115 226 197  51\n",
      "   0 129 188 249 216 165 114 139 224  95  94  33 142  98  86  28 218 252\n",
      "  66  76  77 207 137 194 171  20  40 124 155 230 227 176 106 162 144 169\n",
      "   9 130  84  81 177 231 113 147  26 109 164   8  88  80 243 111 170  29\n",
      " 126  97 174  49 255  59  99  13  78 117 154 150  48 161 185 198 204  58\n",
      " 238 125  79  16   6 119  37 133  64 131 200  12 148 217  65 247 213 187\n",
      " 192 201 110 121 222  53  14  34  19   4  43  11  62  56  69 196 251  31\n",
      " 120  54 234 239 178 132 145  52]\n",
      "No updates to index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/mvseg-ordering/experiments/dataset/multisegment2d.py:156: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"label_type\"].fillna(\"soft\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered task_df: 1248\n",
      "got task df: 1248\n",
      "target_datasets: 1248\n",
      "[44 45  7 29 55  2 13 40 38 26 36 39 18 10 33 57 24 59 22 35 54 56 27 15\n",
      " 16  6 23 28 46 37  3  8 12 20 25  1 21  0  9 30 14 34 19 51 43 11  5 53\n",
      " 49 31 42 58 50  4 41 48 32 17 47 52]\n",
      "Loaded 10 images across 3 MegaMedical targets\n"
     ]
    }
   ],
   "source": [
    "# --- Load images across MegaMedical target indices ---\n",
    "\n",
    "def load_target_images(target_index, n_samples, *, split='train', seed=23):\n",
    "    ds = MegaMedicalDataset(dataset_target=target_index, split=split, seed=seed)\n",
    "    indices = ds.get_data_indices()\n",
    "    print(indices)\n",
    "    rng = np.random.default_rng(seed + int(target_index))\n",
    "    if n_samples > len(indices):\n",
    "        n_samples = len(indices)\n",
    "    pick = rng.choice(indices, size=n_samples, replace=False).tolist()\n",
    "    images = []\n",
    "    for idx in pick:\n",
    "        img, _ = ds.get_item_by_data_index(idx)\n",
    "        images.append(img)\n",
    "    return images, pick\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "all_meta = []\n",
    "\n",
    "for target_index, n_samples in mega_targets:\n",
    "    imgs, indices = load_target_images(target_index, n_samples, seed=seed)\n",
    "    all_images.extend(imgs)\n",
    "    all_labels.extend([f\"target_{target_index}\"] * len(imgs))\n",
    "    for idx in indices:\n",
    "        all_meta.append({\n",
    "            'mega_target_index': int(target_index),\n",
    "            'index': int(idx),\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(all_images)} images across {len(mega_targets)} MegaMedical targets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize selected images ---\n",
    "n = len(all_images)\n",
    "cols = min(5, n)\n",
    "rows = (n + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "if rows == 1:\n",
    "    axes = [axes] if cols == 1 else axes\n",
    "axes_flat = axes if isinstance(axes, (list, tuple)) else axes.flatten()\n",
    "for i in range(rows * cols):\n",
    "    ax = axes_flat[i]\n",
    "    if i < n:\n",
    "        img = all_images[i].detach().cpu().squeeze().numpy()\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(all_labels[i])\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build encoders ---\n",
    "encoders = {\n",
    "    'multiverseg': MultiverSegEncoder(pooling='gap_gmp'),\n",
    "    'clip': CLIPEncoder(model_name='ViT-B-32', pretrained='openai'),\n",
    "    'vit': ViTEncoder(model_name='vit_b_16', pretrained=True),\n",
    "    'dinov2': DinoV2Encoder(model_name='facebook/dinov2-base'),\n",
    "    'medsam': MedSAMEncoder(model_type='vit_b'),\n",
    "}\n",
    "\n",
    "for k in encoders:\n",
    "    encoders[k] = encoders[k].to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         embs\u001b[38;5;241m.\u001b[39mappend(emb\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(embs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m {name: embed_images(enc, all_images) \u001b[38;5;28;01mfor\u001b[39;00m name, enc \u001b[38;5;129;01min\u001b[39;00m encoders\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m         embs\u001b[38;5;241m.\u001b[39mappend(emb\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(embs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m {name: \u001b[43membed_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, enc \u001b[38;5;129;01min\u001b[39;00m encoders\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36membed_images\u001b[0;34m(encoder, images)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emb\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m         emb \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ddmg/mvseg-ordering/experiments/encoders/medsam.py:56\u001b[0m, in \u001b[0;36mMedSAMEncoder.forward\u001b[0;34m(self, image, support_images, support_labels)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAM model is missing preprocess().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(image)\n\u001b[0;32m---> 56\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:180\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m     x \u001b[38;5;241m=\u001b[39m window_unpartition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, pad_hw, (H, W))\n\u001b[1;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/mvseg-ordering-env/lib/python3.10/site-packages/segment_anything/modeling/common.py:26\u001b[0m, in \u001b[0;36mMLPBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Compute embeddings ---\n",
    "@torch.no_grad()\n",
    "def embed_images(encoder, images):\n",
    "    embs = []\n",
    "    for img in images:\n",
    "        img = img.to(device)\n",
    "        emb = encoder(img)\n",
    "        if emb.dim() > 1:\n",
    "            emb = emb.squeeze(0)\n",
    "        embs.append(emb.detach().cpu().numpy())\n",
    "    return np.stack(embs, axis=0)\n",
    "\n",
    "embeddings = {name: embed_images(enc, all_images) for name, enc in encoders.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA + KMeans visualization ---\n",
    "\n",
    "label_names = sorted(set(all_labels))\n",
    "label_to_id = {l: i for i, l in enumerate(label_names)}\n",
    "colors = [label_to_id[l] for l in all_labels]\n",
    "\n",
    "for name, emb in embeddings.items():\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    emb2 = pca.fit_transform(emb)\n",
    "\n",
    "    # k-means clustering (k = number of datasets)\n",
    "    k = len(label_names)\n",
    "    km = KMeans(n_clusters=k, random_state=seed, n_init='auto')\n",
    "    clusters = km.fit_predict(emb2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].scatter(emb2[:, 0], emb2[:, 1], c=colors, cmap='tab10')\n",
    "    axes[0].set_title(f\"{name} PCA (colored by dataset)\")\n",
    "    axes[1].scatter(emb2[:, 0], emb2[:, 1], c=clusters, cmap='tab10')\n",
    "    axes[1].set_title(f\"{name} PCA (k-means clusters)\")\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-cluster image thumbnails (for each encoder) ---\n",
    "# Shows all images per cluster.\n",
    "for encoder_name, emb in embeddings.items():\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    emb2 = pca.fit_transform(emb)\n",
    "    k = len(set(all_labels))\n",
    "    km = KMeans(n_clusters=k, random_state=seed, n_init='auto')\n",
    "    clusters = km.fit_predict(emb2)\n",
    "\n",
    "    cluster_ids = sorted(set(clusters))\n",
    "    max_per_cluster = max((clusters == cid).sum() for cid in cluster_ids)\n",
    "    fig, axes = plt.subplots(len(cluster_ids), max_per_cluster, figsize=(3 * max_per_cluster, 3 * len(cluster_ids)))\n",
    "    if len(cluster_ids) == 1:\n",
    "        axes = [axes]\n",
    "    for row, cid in enumerate(cluster_ids):\n",
    "        idxs = [i for i, c in enumerate(clusters) if c == cid]\n",
    "        for col in range(max_per_cluster):\n",
    "            ax = axes[row][col] if len(cluster_ids) > 1 else axes[col]\n",
    "            if col < len(idxs):\n",
    "                img = all_images[idxs[col]].detach().cpu().squeeze().numpy()\n",
    "                ax.imshow(img, cmap='gray')\n",
    "                ax.set_title(f\"cluster {cid}\")\n",
    "            ax.axis('off')\n",
    "    fig.suptitle(f'Cluster thumbnails ({encoder_name})', y=1.02)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "- Edit `task_specs` to select different tasks/labels/slices.\n",
    "- If you want more samples, increase `n_samples` per task.\n",
    "- CLIP/ViT/DINOv2 can be slow on CPU; use GPU if available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mvseg-ordering-env)",
   "language": "python",
   "name": "mvseg-ordering-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
